{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6548d2c",
   "metadata": {},
   "source": [
    "# Section A: Spark RDD API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e19f7b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from operator import add\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7e3649b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/06 08:26:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark_session = (\n",
    "    SparkSession.builder.master(\"spark://localhost:7077\")\n",
    "    .appName(\"MJEnrico_A3_A\")\n",
    "    .config(\"spark.dynamicAllocation.enabled\", True)\n",
    "    .config(\"spark.dynamicAllocation.shuffleTracking.enabled\", True)\n",
    "    .config(\"spark.dynamicAllocation.executorIdleTimeout\", \"30s\")\n",
    "#     .config(\"spark.shuffle.service.enabled\", True)\n",
    "    .config(\"spark.cores.max\", 4)\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark_context = spark_session.sparkContext\n",
    "spark_context.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14a39e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark_context.parallelize([1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27f0cd96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc55849",
   "metadata": {},
   "source": [
    "### Question A.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d28fc4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:===================>                                       (1 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of german lines  : 1920209\n",
      "Number of english lines : 1920209\n",
      "Num partition german  : 3\n",
      "Num partition english : 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "lines_de = spark_context.textFile(\"hdfs://192.168.2.70:9000/europarl/*de-en.de\")\n",
    "lines_en = spark_context.textFile(\"hdfs://192.168.2.70:9000/europarl/*de-en.en\")\n",
    "\n",
    "n_lines_de = lines_de.count()\n",
    "n_lines_en = lines_en.count()\n",
    "\n",
    "print(f\"Number of german lines  : {n_lines_de}\")\n",
    "print(f\"Number of english lines : {n_lines_en}\")\n",
    "print(f\"Num partition german  : {lines_de.getNumPartitions()}\")\n",
    "print(f\"Num partition english : {lines_en.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ec2eac",
   "metadata": {},
   "source": [
    "### Question A.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28d0c72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocess(sentence):\n",
    "    return sentence.lower().split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1276986c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Resumption of the session', 'I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.', \"Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['resumption', 'of', 'the', 'session'], ['i', 'declare', 'resumed', 'the', 'session', 'of', 'the', 'european', 'parliament', 'adjourned', 'on', 'friday', '17', 'december', '1999,', 'and', 'i', 'would', 'like', 'once', 'again', 'to', 'wish', 'you', 'a', 'happy', 'new', 'year', 'in', 'the', 'hope', 'that', 'you', 'enjoyed', 'a', 'pleasant', 'festive', 'period.'], ['although,', 'as', 'you', 'will', 'have', 'seen,', 'the', 'dreaded', \"'millennium\", \"bug'\", 'failed', 'to', 'materialise,', 'still', 'the', 'people', 'in', 'a', 'number', 'of', 'countries', 'suffered', 'a', 'series', 'of', 'natural', 'disasters', 'that', 'truly', 'were', 'dreadful.']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(lines_en.take(3))\n",
    "en_token = lines_en.map(text_preprocess)\n",
    "print(en_token.take(3))\n",
    "# print(en_token.map(lambda x: 1).reduce(add))  # to confirm number of lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aba3ecd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 4:===================>                                       (1 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of lines after tokenization: 1920209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(f\"Num of lines after tokenization: {en_token.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0eb6324",
   "metadata": {},
   "source": [
    "### Question A.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91da5a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('the', 3663118),\n",
       " ('of', 1736975),\n",
       " ('to', 1611788),\n",
       " ('and', 1345072),\n",
       " ('in', 1134025),\n",
       " ('that', 835871),\n",
       " ('a', 810540),\n",
       " ('is', 792564),\n",
       " ('for', 557349),\n",
       " ('we', 551243)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_word_count = (\n",
    "    en_token.flatMap(lambda x: x)\n",
    "    .map(lambda x: (x, 1))\n",
    "    .reduceByKey(add)\n",
    "    .sortBy(lambda x: x[1], ascending=False)\n",
    ")\n",
    "en_word_count.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979d339b",
   "metadata": {},
   "source": [
    "### Question A.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de692e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_preprocessing(tup):\n",
    "    ID = tup[1]\n",
    "    text = re.sub(f\"[{string.punctuation}]+\", \" \", tup[0])\n",
    "    text = re.sub(f\"[{string.whitespace}]+\", \" \", text)\n",
    "    text = text.lower().strip()\n",
    "    return (ID, text)\n",
    "\n",
    "def pair_filter(tup):\n",
    "    len1, len2 = list(map(lambda x: len(x.split()), tup[1]))\n",
    "    if 0 < len1 < 7 and 0 < len2 < 7 and len1 == len2:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def pair_mapper(tup):\n",
    "    s1, s2 = tup[1]\n",
    "    s1 = s1.split()\n",
    "    s2 = s2.split()\n",
    "    return zip(s1, s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5afb2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "english_rdd = lines_en.zipWithIndex().map(sentence_preprocessing)\n",
    "german_rdd = lines_de.zipWithIndex().map(sentence_preprocessing)\n",
    "\n",
    "join_rdd = (\n",
    "    english_rdd.join(german_rdd)\n",
    "    .filter(pair_filter)\n",
    "    .flatMap(pair_mapper)\n",
    "    .map(lambda x: (x, 1))\n",
    "    .reduceByKey(add)\n",
    "    .sortBy(lambda x: x[1], ascending=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d6ba1d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('is', 'ist'), 9132),\n",
       " (('the', 'die'), 6312),\n",
       " (('debate', 'aussprache'), 4562),\n",
       " (('closed', 'geschlossen'), 4362),\n",
       " (('applause', 'beifall'), 3589),\n",
       " (('that', 'das'), 2161),\n",
       " (('we', 'wir'), 2041),\n",
       " (('the', 'der'), 1828),\n",
       " (('i', 'ich'), 1618),\n",
       " (('you', 'dank'), 1480),\n",
       " (('thank', 'vielen'), 1468),\n",
       " (('this', 'das'), 1281),\n",
       " (('vote', 'abstimmung'), 1266),\n",
       " (('not', 'nicht'), 1141),\n",
       " (('written', 'schriftliche'), 1048),\n",
       " (('mr', 'herr'), 1015),\n",
       " (('rule', 'artikel'), 874),\n",
       " (('are', 'sind'), 843),\n",
       " (('statements', 'erklärungen'), 820),\n",
       " (('a', 'ein'), 721),\n",
       " (('and', 'und'), 696),\n",
       " (('before', 'vor'), 691),\n",
       " (('a', 'eine'), 665),\n",
       " (('minutes', 'protokoll'), 641),\n",
       " (('see', 'siehe'), 632),\n",
       " (('the', 'das'), 576),\n",
       " (('what', 'was'), 558),\n",
       " (('this', 'dies'), 557),\n",
       " (('why', 'warum'), 551),\n",
       " (('report', 'bericht'), 542),\n",
       " (('it', 'es'), 528),\n",
       " (('a5', 'a5'), 498),\n",
       " (('have', 'haben'), 464),\n",
       " (('142', '142'), 453),\n",
       " (('you', 'sie'), 450),\n",
       " (('it', 'das'), 447),\n",
       " (('for', 'für'), 446),\n",
       " (('149', '149'), 434),\n",
       " (('president', 'präsident'), 411),\n",
       " (('on', 'über'), 411),\n",
       " (('amendment', 'änderungsantrag'), 409),\n",
       " (('is', 'wird'), 402),\n",
       " (('very', 'sehr'), 391),\n",
       " (('there', 'es'), 384),\n",
       " (('they', 'sie'), 371),\n",
       " (('of', 'der'), 333),\n",
       " (('was', 'war'), 328),\n",
       " (('has', 'hat'), 317),\n",
       " (('in', 'in'), 316),\n",
       " (('must', 'müssen'), 313)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "join_rdd.take(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4d0f88",
   "metadata": {},
   "source": [
    "### My comment: \n",
    "\n",
    "The paradigm performs reasonably well for a simple translaton method like this. Only few words are wrongly translated such as \"you\" being translated to \"dank\". However, the downside of using this technique is its inability to translate less frequently mentioned words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5df25d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_context.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba698696",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
